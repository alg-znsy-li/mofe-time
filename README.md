# mofe-time

## æ‘˜è¦
åœ¨è¿‡å»çš„å‡ åå¹´ï¼Œæ—¶é—´åºåˆ—é¢„æµ‹æŠ€æœ¯å–å¾—äº†å·¨å¤§çš„æŠ€æœ¯è¿›æ­¥ï¼Œç‰¹åˆ«æ˜¯åŸºäºé¢„è®­ç»ƒæ–¹æ³•çš„æ—¶é—´åºåˆ—é¢„æµ‹æŠ€æœ¯å‘å±•éå¸¸è¿…é€Ÿã€‚è¿™äº›æŠ€æœ¯ä¸»è¦åŒ…æ‹¬LLM4TSå’ŒTSLLMä¸¤ä¸ªæŠ€æœ¯æ–¹å‘ï¼Œç ”ç©¶è®¤ä¸ºæ—¶é—´åºåˆ—æ˜¯ä¸€ç§å¯ä»¥å­¦ä¹ çš„è¯­è¨€â€”â€”ç”¨LLMè¾…åŠ©æˆ–è€…ç›´æ¥å»ºæ¨¡æ—¶é—´åºåˆ—ã€‚ç„¶è€Œæ—¶é—´åºåˆ—æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªç¦»æ•£çš„ä¿¡å·ï¼Œç°æœ‰çš„ä¸Šè¿°ä¸¤ç§å»ºæ¨¡æ€è·¯ï¼Œå¹¶æ²¡æœ‰å……åˆ†è€ƒè™‘æ—¶é—´åºåˆ—çš„ä¿¡å·æœ¬å¾å±æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºMoEæ¶æ„çš„é¢‘åŸŸå­¦ä¹ çš„åŸºç¡€æ—¶åºå¤§æ¨¡å‹â€”â€”MoFE-Timeã€‚è¿™ç§æŠ€æœ¯å¼•å…¥äº†æœ€æ–°çš„ä¿¡å·å­¦ä¹ ç½‘ç»œFANï¼ˆå‚…é‡Œå¶åˆ†æç½‘ç»œï¼‰å’Œé¢†å…ˆçš„LLMåŸºç¡€æ¶æ„MoEï¼Œåˆ›æ–°æ€§åœ°å°†é¢‘åŸŸä¸“å®¶å¼•å…¥MoEæ¶æ„ä¸­ï¼Œåˆ©ç”¨MoEçš„è·¯ç”±ç®—æ³•ï¼Œä½¿å¾—ç®—æ³•å¯ä»¥å…³æ³¨ä¸åŒçš„é¢‘åŸŸå±æ€§ï¼Œä»è€Œæ›´å¥½çš„å»ºæ¨¡æ—¶é—´åºåˆ—çš„é¢‘åŸŸæœ¬å¾å±æ€§ã€‚æˆ‘ä»¬å‘ç°ç›¸æ¯”ç›®å‰æ—¶åŸŸæœ€å¥½çš„å¼€æºå¤§æ¨¡å‹ï¼šï¼ˆ1ï¼‰åœ¨å¤šä¸ªä¸åŒçš„æ•°æ®é›†ä¸­ï¼ŒMoFE-Timeæ¨¡å‹å‡ ä¹éƒ½å…·æœ‰æ›´å¥½çš„æ€§èƒ½è¡¨ç°ã€‚ï¼ˆ2ï¼‰è¾¾åˆ°åŒç­‰çš„é›¶æ ·æœ¬é¢„æµ‹æ•ˆæœæ—¶ï¼ŒMoFE-Timeå…·æœ‰æ›´ä½çš„è®­ç»ƒæˆæœ¬å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼›ï¼ˆ3ï¼‰MoFE-Time å…·æœ‰æ›´å¥½çš„åè®­ç»ƒç‰¹æ€§, å¾®è°ƒåæ¨¡å‹æ€§èƒ½åœ¨å¤šä¸ªå…¬å¼€æ•°æ®é›†ä¸Šå‡å¤§å¹…é¢†å…ˆç°æœ‰çš„æ—¶åºæ¨¡å‹ã€‚ æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬çš„å·¥ä½œæ˜¯é¦–æ¬¡åœ¨æ—¶é—´åºåˆ—åŸºç¡€å¤§æ¨¡å‹ä¸­ï¼Œç»“åˆMoEå’Œä¿¡å·é¢‘åŸŸæœ¬å¾å±æ€§å­¦ä¹ çš„æŠ€æœ¯ã€‚

## æŠ€æœ¯åŸç†
paper: https://arxiv.org/abs/2507.06502

<img width="553" alt="image" src="https://github.com/user-attachments/assets/c3ffe966-2556-4340-8d0b-a5653ad95eb0" />

## ä¸»è¦æŠ€æœ¯æŒ‡æ ‡
### zero-shot


### full-shot
<img width="461" alt="image" src="https://github.com/user-attachments/assets/410fe74b-06cd-4a5c-a43f-fe39f95e17a8" />

## Usage
### pretrain
1. data prepare
   
   ![image](https://github.com/user-attachments/assets/4b17181a-6ff7-480f-b66a-e4116d615c04)

   download from huggingface
   https://huggingface.co/datasets/Maple728/Time-300B
3. start pretrain
    ```./```

### fine tune

### infer

## Citation

> ğŸ™‹ Please let us know if you find out a mistake or have any suggestions!

> ğŸŒŸ If you find the MOFE-Time models helpful in your research, please consider to star this repository and cite the
> corresponding

```
@misc{liu2025mofetimemixturefrequencydomain,
      title={MoFE-Time: Mixture of Frequency Domain Experts for Time-Series Forecasting Models}, 
      author={Yiwen Liu and Chenyu Zhang and Junjie Song and Siqi Chen and Sun Yin and Zihan Wang and Lingming Zeng and Yuji Cao and Junming Jiao},
      year={2025},
      eprint={2507.06502},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2507.06502}, 
}
```

## Related Resources
* TimeMixer++: A General Time Series Pattern Machine for Universal Predictive Analysis, in arXiv 2024. [\[paper\]](https://arxiv.org/abs/2410.16032) [\[GitHub Repo\]](https://github.com/kwuking/TimeMixer)
* Towards Neural Scaling Laws for Time Series Foundation Models, arXiv 2024. [\[paper\]](https://arxiv.org/pdf/2410.12360)
* Foundation Models for Time Series Analysis: A Tutorial and Survey, in *KDD*
  2024. [\[paper\]](https://arxiv.org/abs/2403.14735) [\[Tutorial\]](https://wenhaomin.github.io/FM4TS.github.io/)
* What Can Large Language Models Tell Us about Time Series Analysis, in *ICML*
  2024. [\[paper\]](https://arxiv.org/abs/2402.02713)
* Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects, in *TPAMI*
  2024. [\[paper\]](https://arxiv.org/abs/2306.10125) [\[Website\]](https://github.com/qingsongedu/Awesome-SSL4TS)
* Transformers in Time Series: A Survey, in *IJCAI*
  2023. [\[paper\]](https://arxiv.org/abs/2202.07125) [\[GitHub Repo\]](https://github.com/qingsongedu/time-series-transformers-review)
* A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection, in *TPAMI* 2024. [\[paper\]](https://arxiv.org/abs/2307.03759) [\[Website\]](https://github.com/KimMeen/Awesome-GNN4TS)


